{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark Initiation and Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment 2\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "from pyspark import SparkConf # Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+----+----------+-------------+--------+------+-------+--------+--------+--------------------+--------------------+-----+---+-----------+-------------+------------+--------------+\n",
      "| ID|Type|Rooms|Year|    Suburb|      Address|   Price|Method|SellerG|Distance|Landsize|         CouncilArea|          Regionname|Month|Day|MedianPrice|AuctionResult|StreetSuffix|binnedDistance|\n",
      "+---+----+-----+----+----------+-------------+--------+------+-------+--------+--------+--------------------+--------------------+-----+---+-----------+-------------+------------+--------------+\n",
      "|  0|   t|    3|2018|point cook|23 Tribeca Dr|535000.0|    SA|  Point|      14|    78.0|wyndham city council|western metropolitan|    2| 17|   592000.0|            0|          Dr|             2|\n",
      "+---+----+-----+----+----------+-------------+--------+------+-------+--------+--------+--------------------+--------------------+-----+---+-----------+-------------+------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimisation step - include schema definitions first before reading in data\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('ID', IntegerType(), True),\n",
    "    StructField('Type', StringType(), True),\n",
    "    StructField('Rooms', IntegerType(), True), \n",
    "    StructField('Year', IntegerType(), True), \n",
    "    StructField('Suburb', StringType(), True),\n",
    "    StructField('Address', StringType(), True),\n",
    "    StructField('Price', DoubleType(), True), \n",
    "    StructField('Method', StringType(), True), \n",
    "    StructField('SellerG', StringType(), True), \n",
    "    StructField('Distance', IntegerType(), True),\n",
    "    StructField('Landsize', DoubleType(), True), \n",
    "    StructField('CouncilArea', StringType(), True), \n",
    "    StructField('Regionname', StringType(), True),\n",
    "    StructField('Month', IntegerType(), True), \n",
    "    StructField('Day', IntegerType(), True), \n",
    "    StructField('MedianPrice', DoubleType(), True), \n",
    "    StructField('AuctionResult', IntegerType(), True),\n",
    "    StructField('StreetSuffix', StringType(), True), \n",
    "    StructField('binnedDistance', IntegerType(), True)\n",
    "])\n",
    "\n",
    "file = \"Dataset_for_Model_1_Price_Prediction.csv\"\n",
    "\n",
    "df = spark.read.csv(file,  \n",
    "    header=True,\n",
    "    schema=schema\n",
    ")\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Rooms: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Method: string (nullable = true)\n",
      " |-- SellerG: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- Landsize: double (nullable = true)\n",
      " |-- CouncilArea: string (nullable = true)\n",
      " |-- Regionname: string (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- MedianPrice: double (nullable = true)\n",
      " |-- AuctionResult: integer (nullable = true)\n",
      " |-- StreetSuffix: string (nullable = true)\n",
      " |-- binnedDistance: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema in tree format \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Remove nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has nulls remaining in it because it will be used for two different models, and we believe that different features will be of varying usefulness for each one (basically we did't want to delete data on account of a column that may not even be used in the end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the null values\n",
    "\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "nulls = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "\n",
      "ID: 0\n",
      "Type: 0\n",
      "Rooms: 0\n",
      "Year: 0\n",
      "Suburb: 0\n",
      "Address: 0\n",
      "Price: 854\n",
      "Method: 0\n",
      "SellerG: 0\n",
      "Distance: 0\n",
      "Landsize: 0\n",
      "CouncilArea: 0\n",
      "Regionname: 0\n",
      "Month: 0\n",
      "Day: 0\n",
      "MedianPrice: 267\n",
      "AuctionResult: 0\n",
      "StreetSuffix: 0\n",
      "binnedDistance: 0\n"
     ]
    }
   ],
   "source": [
    "# Look at the missing values across the columns\n",
    "\n",
    "print(\"Missing values:\\n\")\n",
    "for i in range(len(df.columns)):\n",
    "    print(f\"{df.columns[i]}: {nulls[0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the nulls\n",
    "\n",
    "df = df.na.drop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+----+------+-------+-----+------+-------+--------+--------+-----------+----------+-----+---+-----------+-------------+------------+--------------+\n",
      "| ID|Type|Rooms|Year|Suburb|Address|Price|Method|SellerG|Distance|Landsize|CouncilArea|Regionname|Month|Day|MedianPrice|AuctionResult|StreetSuffix|binnedDistance|\n",
      "+---+----+-----+----+------+-------+-----+------+-------+--------+--------+-----------+----------+-----+---+-----------+-------------+------------+--------------+\n",
      "|  0|   0|    0|   0|     0|      0|    0|     0|      0|       0|       0|          0|         0|    0|  0|          0|            0|           0|             0|\n",
      "+---+----+-----+----+------+-------+-----+------+-------+--------+--------+-----------+----------+-----+---+-----------+-------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the dataframe after dropping null values\n",
    "\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'int'),\n",
       " ('Type', 'string'),\n",
       " ('Rooms', 'int'),\n",
       " ('Year', 'int'),\n",
       " ('Suburb', 'string'),\n",
       " ('Address', 'string'),\n",
       " ('Price', 'double'),\n",
       " ('Method', 'string'),\n",
       " ('SellerG', 'string'),\n",
       " ('Distance', 'int'),\n",
       " ('Landsize', 'double'),\n",
       " ('CouncilArea', 'string'),\n",
       " ('Regionname', 'string'),\n",
       " ('Month', 'int'),\n",
       " ('Day', 'int'),\n",
       " ('MedianPrice', 'double'),\n",
       " ('AuctionResult', 'int'),\n",
       " ('StreetSuffix', 'string'),\n",
       " ('binnedDistance', 'int')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features must be defined as numeric or categoric so they can be treated appropriately by the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryInputCols = ['Type', \"Suburb\", \"Method\", \"SellerG\", \"CouncilArea\", \"Regionname\", \"StreetSuffix\"]\n",
    "numericInputCols = [\"Rooms\", \"Year\", \"Distance\", \"Landsize\", \"Month\", \"Day\", \"MedianPrice\", \"AuctionResult\"]\n",
    "\n",
    "# The response var\n",
    "numericOutputCol = 'Price'\n",
    "\n",
    "categoryCols = categoryInputCols\n",
    "numericCols = numericInputCols+[numericOutputCol]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OHE: To convert categorical values into numeric values, we use a technique called one-hot encoding (OHE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexer: A label indexer that maps a string column of labels to an ML column of label indices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input column is numeric, we cast it to integer and index the string values. The indices are in [0, numLabels). By default, this is ordered by label frequencies so the most frequent label gets index 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the output columns\n",
    "\n",
    "outputCols=[f'{x}_index' for x in categoryInputCols]\n",
    "inputIndexer = StringIndexer(inputCols=categoryCols, outputCols=outputCols).setHandleInvalid(\"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+------------+-------------+-----------------+----------------+------------------+\n",
      "|Type_index|Suburb_index|Method_index|SellerG_index|CouncilArea_index|Regionname_index|StreetSuffix_index|\n",
      "+----------+------------+------------+-------------+-----------------+----------------+------------------+\n",
      "|       2.0|        84.0|         6.0|         96.0|             19.0|             2.0|               4.0|\n",
      "|       2.0|        84.0|         0.0|         96.0|             19.0|             2.0|              13.0|\n",
      "+----------+------------+------------+-------------+-----------------+----------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply indexing to categoric data\n",
    "df_indexed = inputIndexer.fit(df).transform(df)\n",
    "\n",
    "# show encoded result\n",
    "df_indexed.select(outputCols).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Encoders are highly specialized and optimized code generators that generate custom bytecode for serialization and deserialization of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputCols_OHE = [x for x in outputCols if x!='label']\n",
    "outputCols_OHE = [f'{x}_vec' for x in categoryInputCols]\n",
    "\n",
    "# Define OneHotEncoder with the appropriate columns\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=inputCols_OHE,outputCols=outputCols_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-------------+----------------+---------------+--------------+----------------+\n",
      "|     Type_vec|      Suburb_vec|   Method_vec|     SellerG_vec|CouncilArea_vec|Regionname_vec|StreetSuffix_vec|\n",
      "+-------------+----------------+-------------+----------------+---------------+--------------+----------------+\n",
      "|(3,[2],[1.0])|(324,[84],[1.0])|(9,[6],[1.0])|(363,[96],[1.0])|(33,[19],[1.0])| (8,[2],[1.0])|  (22,[4],[1.0])|\n",
      "|(3,[2],[1.0])|(324,[84],[1.0])|(9,[0],[1.0])|(363,[96],[1.0])|(33,[19],[1.0])| (8,[2],[1.0])| (22,[13],[1.0])|\n",
      "+-------------+----------------+-------------+----------------+---------------+--------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = encoder.fit(df_indexed)\n",
    "# Call fit and transform to get the encoded results\n",
    "df_encoded = model.transform(df_indexed)\n",
    "# Display the output columns\n",
    "df_encoded.select(outputCols_OHE).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assembler: A feature transformer that merges multiple columns into a vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputCols are all the encoded columns from OHE plus numerical columns\n",
    "inputCols=outputCols_OHE\n",
    "assemblerInputs = outputCols_OHE + numericInputCols\n",
    "\n",
    "# Define the assembler with appropriate input and output columns\n",
    "assembler = VectorAssembler(inputCols = assemblerInputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(770,[2,87,333,43...|\n",
      "|(770,[2,87,327,43...|\n",
      "|(770,[2,87,329,39...|\n",
      "|(770,[0,40,327,36...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,328,33...|\n",
      "|(770,[0,40,327,34...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,329,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,329,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,330,33...|\n",
      "|(770,[0,40,329,33...|\n",
      "|(770,[0,40,330,33...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the asseembler transform() to get encoded results\n",
    "df_assembled = assembler.transform(df_encoded)\n",
    "\n",
    "# Display the output\n",
    "df_assembled.select('features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizes features by removing the mean and scaling to unit variance using column summary statistics on the samples in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "standard_scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = standard_scaler.fit(df_assembled).transform(df_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              scaled|\n",
      "+--------------------+\n",
      "|(770,[2,87,333,43...|\n",
      "|(770,[2,87,327,43...|\n",
      "|(770,[2,87,329,39...|\n",
      "|(770,[0,40,327,36...|\n",
      "|(770,[0,40,327,33...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scaled.select('scaled').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines and PipelineModels help to ensure that training and test data go through identical feature processing steps.\n",
    "stage_1 = inputIndexer\n",
    "stage_2 = encoder\n",
    "stage_3 = assembler\n",
    "stage_4 = standard_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stages = [stage_1,stage_2,stage_3, stage_4, stage_5]\n",
    "stages = [inputIndexer,encoder,assembler,standard_scaler ]\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df_pipeline = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Creating Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22097 rows in the training set,\n",
      "and 9397 in the test set\n"
     ]
    }
   ],
   "source": [
    "# Divide data into train sets and test sets. \n",
    "# Seed is the value used to make the same data three times later\n",
    "train_df, test_df = df_pipeline.randomSplit([.7, .3], seed=42)\n",
    "print(f\"\"\"There are {train_df.count()} rows in the training set,\n",
    "and {test_df.count()} in the test set\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression models a linear relationship between the dependent variable (or\n",
    "label) and one or more independent variables (or features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol='scaled', labelCol='Price', ) \n",
    "lrModel = lr.fit(train_df)\n",
    "#lrModel\n",
    "#print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "#print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 345780.558716\n",
      "r2: 0.730503\n"
     ]
    }
   ],
   "source": [
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To fit a linear regression model to predict the price of the house given the number of rooms.\n",
    "- we have a single feature x and an output y (this is our dependent variable).\n",
    "- Linear regression seeks to fit an equation for a line to x and y, which for scalar variables can be expressed as y = mx + b, where m is the slope and b is the offset or intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression line is\n",
      "price = 92777.33*Rooms + -142924521.32\n"
     ]
    }
   ],
   "source": [
    "# Creating the linear regression line\n",
    "m = round(lrModel.coefficients[0], 2)\n",
    "b = round(lrModel.intercept, 2)\n",
    "print(f\"\"\"The formula for the linear regression line is\n",
    "price = {m}*Rooms + {b}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative intercept value indicates that when we extend the regression line downwards until we reach the point where it crosses the y-axis and also it is difficult to interpret the constant term because the y-intercept is almost meaningless, yet it is always vital to include it in regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model on test data set\n",
    "lrPredictions = lrModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------------+\n",
      "|              scaled|    Price|       prediction|\n",
      "+--------------------+---------+-----------------+\n",
      "|(770,[2,87,329,39...| 597817.0|97148.55679512024|\n",
      "|(770,[0,40,328,33...| 900000.0|709556.1500967145|\n",
      "|(770,[0,40,327,33...|1315000.0| 879538.629814595|\n",
      "|(770,[0,40,329,33...| 440000.0|785428.0889554322|\n",
      "|(770,[0,40,327,33...|1051000.0|924063.7782126069|\n",
      "|(770,[0,40,329,33...| 720000.0|912311.8418701887|\n",
      "|(770,[0,40,327,33...| 590069.0|935910.1229119599|\n",
      "|(770,[0,40,330,33...| 545000.0|829006.8728214502|\n",
      "|(770,[0,40,330,33...| 465000.0|904364.2704406679|\n",
      "|(770,[0,40,329,33...| 825000.0|800162.2443299592|\n",
      "+--------------------+---------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrPredictions.select('scaled','Price','prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built a model, we need to evaluate how well it performs. In spark.ml there are classification, regression, clustering, and ranking evaluators. Given that this is a regression problem, we will use rootmean- square error (RMSE) and R2 (pronounced “R-squared”) to evaluate our model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2 (R-Squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) on test data = 0.691328\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"Price\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lrPredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R squared at 0.69 indicates that in our model, approximate 69% of the variability in “Price” can be explained using the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE\n",
    "Evaluator for Regression, which expects input columns prediction, label and an optional weight column. RMSE is a metric that ranges from zero to infinity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 369786\n"
     ]
    }
   ],
   "source": [
    "test_result = lrModel.evaluate(test_df)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % test_result.rootMeanSquaredError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fine tune the models using grid search technique which will evaluate all the possible combinations of hyperparameters values using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pipeline estimator\n",
    "pipeline = Pipeline(stages = [inputIndexer,encoder,assembler, standard_scaler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator,CrossValidatorModel\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Create ParamGrid for Cross Validation\n",
    "# Specifying the hyperparameters and their respective values using the ParamGridBuilder\n",
    "paramGrid = (ParamGridBuilder()\n",
    ".addGrid(lr.maxIter, [1000])\n",
    ".addGrid(lr.regParam, [0.1,0.5, 1.0, 2.0])\n",
    ".addGrid(lr.elasticNetParam, [0.3,0.6, 0.9])\n",
    ".build())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining how to evaluate each of the models to determine which one perfroms best\n",
    "# Using RegressionEvaluator and RMSE as the metric\n",
    "evaluator = RegressionEvaluator(labelCol=\"Price\",\n",
    "predictionCol=\"prediction\",\n",
    "metricName=\"rmse\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform the k-fold cross-validation using the CrossValidator, which accepts an estimator, evaluator, and estimatorParamMaps so that it knows which model to use, how to evaluate the model, and which hyperparameters to set for the model. We also set the number of folds we want to split the data into (numFolds=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=lr,\n",
    "evaluator=evaluator,\n",
    "estimatorParamMaps=paramGrid,\n",
    "numFolds=3,\n",
    "parallelism = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidatorModel_6a52593c4b67\n"
     ]
    }
   ],
   "source": [
    "# Fitting the cvModel to train data set\n",
    "cvModel = cv.fit(train_df)\n",
    "print(cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------------+\n",
      "|              scaled|    Price|       prediction|\n",
      "+--------------------+---------+-----------------+\n",
      "|(770,[2,87,329,39...| 597817.0| 98250.3813854903|\n",
      "|(770,[0,40,328,33...| 900000.0|710048.8479898721|\n",
      "|(770,[0,40,327,33...|1315000.0|879680.9555134624|\n",
      "|(770,[0,40,329,33...| 440000.0|786156.5398058295|\n",
      "|(770,[0,40,327,33...|1051000.0|924506.0409045517|\n",
      "|(770,[0,40,329,33...| 720000.0| 912652.246259138|\n",
      "|(770,[0,40,327,33...| 590069.0|936321.5586847812|\n",
      "|(770,[0,40,330,33...| 545000.0|829428.3790077269|\n",
      "|(770,[0,40,330,33...| 465000.0| 904236.783978954|\n",
      "|(770,[0,40,329,33...| 825000.0|800112.5389250219|\n",
      "+--------------------+---------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on test data set\n",
    "lrPredictions = bestModel.transform(test_df)\n",
    "lrPredictions.select('scaled','Price','prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 369802.8\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "predictionCol=\"prediction\",\n",
    "labelCol=\"Price\",\n",
    "metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(lrPredictions)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.6913281685948564\n"
     ]
    }
   ],
   "source": [
    "# Calculating the R-squared value\n",
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(lrPredictions)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation and Hyperparameter Tuning methods didn't improve the Linear Regression Model performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
