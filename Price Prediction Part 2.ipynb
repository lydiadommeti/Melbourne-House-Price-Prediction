{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyspark Initiation and Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# local[*]: run Spark in local mode with as many working processors as logical cores on your machine\n",
    "# If we want Spark to run locally with 'k' worker threads, we can specify as \"local[k]\".\n",
    "master = \"local[*]\"\n",
    "# The `appName` field is a name to be shown on the Spark cluster UI page\n",
    "app_name = \"Assignment 2\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SparkContext and SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "\n",
    "# Using SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+----+----------+-------------+--------+------+-------+--------+--------+--------------------+--------------------+-----+---+-----------+-------------+------------+--------------+\n",
      "| ID|Type|Rooms|Year|    Suburb|      Address|   Price|Method|SellerG|Distance|Landsize|         CouncilArea|          Regionname|Month|Day|MedianPrice|AuctionResult|StreetSuffix|binnedDistance|\n",
      "+---+----+-----+----+----------+-------------+--------+------+-------+--------+--------+--------------------+--------------------+-----+---+-----------+-------------+------------+--------------+\n",
      "|  0|   t|    3|2018|point cook|23 Tribeca Dr|535000.0|    SA|  Point|      14|    78.0|wyndham city council|western metropolitan|    2| 17|   592000.0|            0|          Dr|             2|\n",
      "+---+----+-----+----+----------+-------------+--------+------+-------+--------+--------+--------------------+--------------------+-----+---+-----------+-------------+------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optimisation step - include schema definitions first before reading in data\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('ID', IntegerType(), True),\n",
    "    StructField('Type', StringType(), True),\n",
    "    StructField('Rooms', IntegerType(), True), \n",
    "    StructField('Year', IntegerType(), True), \n",
    "    StructField('Suburb', StringType(), True),\n",
    "    StructField('Address', StringType(), True),\n",
    "    StructField('Price', DoubleType(), True), \n",
    "    StructField('Method', StringType(), True), \n",
    "    StructField('SellerG', StringType(), True), \n",
    "    StructField('Distance', IntegerType(), True),\n",
    "    StructField('Landsize', DoubleType(), True), \n",
    "    StructField('CouncilArea', StringType(), True), \n",
    "    StructField('Regionname', StringType(), True),\n",
    "    StructField('Month', IntegerType(), True), \n",
    "    StructField('Day', IntegerType(), True), \n",
    "    StructField('MedianPrice', DoubleType(), True), \n",
    "    StructField('AuctionResult', IntegerType(), True),\n",
    "    StructField('StreetSuffix', StringType(), True), \n",
    "    StructField('binnedDistance', IntegerType(), True)\n",
    "])\n",
    "\n",
    "file = \"Dataset_for_Model_1_Price_Prediction.csv\"\n",
    "\n",
    "df = spark.read.csv(file,  \n",
    "    header=True,\n",
    "    schema=schema\n",
    ")\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: int, Type: string, Rooms: int, Year: int, Suburb: string, Price: double, Method: string, SellerG: string, Distance: int, Landsize: double, CouncilArea: string, Regionname: string, Month: int, Day: int, MedianPrice: double, AuctionResult: int, StreetSuffix: string, binnedDistance: int]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the address column, it will not be useful for learning from\n",
    "df.drop('Address')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Rooms: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Price: double (nullable = true)\n",
      " |-- Method: string (nullable = true)\n",
      " |-- SellerG: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- Landsize: double (nullable = true)\n",
      " |-- CouncilArea: string (nullable = true)\n",
      " |-- Regionname: string (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- MedianPrice: double (nullable = true)\n",
      " |-- AuctionResult: integer (nullable = true)\n",
      " |-- StreetSuffix: string (nullable = true)\n",
      " |-- binnedDistance: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema in tree format \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove nulls\n",
    "This dataset has nulls reamining in it because it will be used for two different models, and we believe that different features will be of varying usefulness for each one (basically we did't want to delete data on account of a column that may not even be used in the end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "nulls = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Year</th>\n",
       "      <th>Suburb</th>\n",
       "      <th>Address</th>\n",
       "      <th>Price</th>\n",
       "      <th>Method</th>\n",
       "      <th>SellerG</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>CouncilArea</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>MedianPrice</th>\n",
       "      <th>AuctionResult</th>\n",
       "      <th>StreetSuffix</th>\n",
       "      <th>binnedDistance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>267</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Type  Rooms  Year  Suburb  Address  Price  Method  SellerG  Distance  \\\n",
       "0   0     0      0     0       0        0    854       0        0         0   \n",
       "\n",
       "   Landsize  CouncilArea  Regionname  Month  Day  MedianPrice  AuctionResult  \\\n",
       "0         0            0           0      0    0          267              0   \n",
       "\n",
       "   StreetSuffix  binnedDistance  \n",
       "0             0               0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the missing values across the columns\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the nulls\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Type</th>\n",
       "      <th>Rooms</th>\n",
       "      <th>Year</th>\n",
       "      <th>Suburb</th>\n",
       "      <th>Address</th>\n",
       "      <th>Price</th>\n",
       "      <th>Method</th>\n",
       "      <th>SellerG</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Landsize</th>\n",
       "      <th>CouncilArea</th>\n",
       "      <th>Regionname</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>MedianPrice</th>\n",
       "      <th>AuctionResult</th>\n",
       "      <th>StreetSuffix</th>\n",
       "      <th>binnedDistance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Type  Rooms  Year  Suburb  Address  Price  Method  SellerG  Distance  \\\n",
       "0   0     0      0     0       0        0      0       0        0         0   \n",
       "\n",
       "   Landsize  CouncilArea  Regionname  Month  Day  MedianPrice  AuctionResult  \\\n",
       "0         0            0           0      0    0            0              0   \n",
       "\n",
       "   StreetSuffix  binnedDistance  \n",
       "0             0               0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for the result after dropping null values\n",
    "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required libraries\n",
    "\n",
    "from pyspark import SparkConf # Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features must be defined as numeric or categoric so they can be treated appropriately by the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryInputCols = ['Type', \"Suburb\", \"Method\", \"SellerG\", \"CouncilArea\", \"Regionname\", \"StreetSuffix\"]\n",
    "numericInputCols = [\"Rooms\", \"Year\", \"Distance\", \"Landsize\", \"Month\", \"Day\", \"MedianPrice\", \"AuctionResult\"]\n",
    "\n",
    "# The response var\n",
    "numericOutputCol = 'Price'\n",
    "\n",
    "categoryCols = categoryInputCols\n",
    "numericCols = numericInputCols+[numericOutputCol]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the string indexer to convert the categoric variables into indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the output columns\n",
    "\n",
    "outputCols=[f'{x}_index' for x in categoryInputCols]\n",
    "inputIndexer = StringIndexer(inputCols=categoryCols, outputCols=outputCols).setHandleInvalid(\"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Type', 'Suburb', 'Method', 'SellerG', 'CouncilArea', 'Regionname', 'StreetSuffix']\n",
      "['Type_index', 'Suburb_index', 'Method_index', 'SellerG_index', 'CouncilArea_index', 'Regionname_index', 'StreetSuffix_index']\n",
      "+----------+------------+------------+-------------+-----------------+----------------+------------------+\n",
      "|Type_index|Suburb_index|Method_index|SellerG_index|CouncilArea_index|Regionname_index|StreetSuffix_index|\n",
      "+----------+------------+------------+-------------+-----------------+----------------+------------------+\n",
      "|       2.0|        84.0|         6.0|         96.0|             19.0|             2.0|               4.0|\n",
      "|       2.0|        84.0|         0.0|         96.0|             19.0|             2.0|              13.0|\n",
      "+----------+------------+------------+-------------+-----------------+----------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(categoryCols)\n",
    "print(outputCols)\n",
    "\n",
    "# apply indexing to categoric data\n",
    "df_indexed = inputIndexer.fit(df).transform(df)\n",
    "\n",
    "# show encoded result\n",
    "df_indexed.select(outputCols).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encode the categoric data, to generate binary vectors for each category value in every categoric feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputCols_OHE = [f'{x}_vec' for x in categoryInputCols]\n",
    "\n",
    "#Define OneHotEncoder with the appropriate columns\n",
    "encoder = OneHotEncoder(inputCols=inputCols_OHE,\n",
    "                        outputCols=outputCols_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-------------+----------------+---------------+--------------+----------------+\n",
      "|     Type_vec|      Suburb_vec|   Method_vec|     SellerG_vec|CouncilArea_vec|Regionname_vec|StreetSuffix_vec|\n",
      "+-------------+----------------+-------------+----------------+---------------+--------------+----------------+\n",
      "|(3,[2],[1.0])|(324,[84],[1.0])|(9,[6],[1.0])|(363,[96],[1.0])|(33,[19],[1.0])| (8,[2],[1.0])|  (22,[4],[1.0])|\n",
      "|(3,[2],[1.0])|(324,[84],[1.0])|(9,[0],[1.0])|(363,[96],[1.0])|(33,[19],[1.0])| (8,[2],[1.0])| (22,[13],[1.0])|\n",
      "+-------------+----------------+-------------+----------------+---------------+--------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = encoder.fit(df_indexed)\n",
    "# Call fit and transform to get the encoded results\n",
    "df_encoded = model.transform(df_indexed)\n",
    "# Display the output columns\n",
    "df_encoded.select(outputCols_OHE).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the input columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputCols are all the encoded columns from OHE plus numerical columns\n",
    "inputCols=outputCols_OHE\n",
    "assemblerInputs = outputCols_OHE + numericInputCols\n",
    "\n",
    "# Define the assembler with appropriate input and output columns\n",
    "assembler = VectorAssembler(inputCols = assemblerInputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(770,[2,87,333,43...|\n",
      "|(770,[2,87,327,43...|\n",
      "|(770,[2,87,329,39...|\n",
      "|(770,[0,40,327,36...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,328,33...|\n",
      "|(770,[0,40,327,34...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,329,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,329,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,327,33...|\n",
      "|(770,[0,40,330,33...|\n",
      "|(770,[0,40,329,33...|\n",
      "|(770,[0,40,330,33...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use the asseembler transform() to get encoded results\n",
    "df_assembled = assembler.transform(df_encoded)\n",
    "\n",
    "# Display the output\n",
    "df_assembled.select('features').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines and PipelineModels help to ensure that training and test data go through identical feature processing steps.\n",
    "stages = [inputIndexer,encoder,assembler]\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df_pipeline = pipelineModel.transform(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training (70%) and testing (30%) datesets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 22166\n",
      "Test Dataset Count: 9328\n"
     ]
    }
   ],
   "source": [
    "# Divide data into train sets and test sets. \n",
    "# Seed is the value used to make the same data three times later\n",
    "train, test = df_pipeline.randomSplit([0.7, 0.3], seed = 2020)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are easy to interpret, handle categorical features, extend to the multiclass classification setting, do not require feature scaling, and are able to capture non-linearities and feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# Extracts the number of nodes in the decision tree and the tree depth in the model and stores it in dt.\n",
    "dt = DecisionTreeRegressor(featuresCol = 'features', labelCol = 'Price')\n",
    "dtModel = dt.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------------+\n",
      "|            features|    Price|        prediction|\n",
      "+--------------------+---------+------------------+\n",
      "|(770,[0,40,327,33...|1315000.0| 832024.4106776181|\n",
      "|(770,[0,40,327,33...| 711000.0| 832024.4106776181|\n",
      "|(770,[0,40,327,33...|1051000.0|1242608.2911714772|\n",
      "|(770,[0,40,327,33...| 590069.0|1242608.2911714772|\n",
      "|(770,[0,40,327,33...| 573000.0| 1029331.357495069|\n",
      "|(770,[0,40,330,33...| 545000.0| 1029331.357495069|\n",
      "|(770,[0,40,328,39...|1225000.0| 1029331.357495069|\n",
      "|(770,[0,40,327,34...| 887500.0| 1029331.357495069|\n",
      "|(770,[0,3,327,341...| 707500.0| 832024.4106776181|\n",
      "|(770,[0,3,328,363...| 801000.0| 832024.4106776181|\n",
      "+--------------------+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dtPredictions = dtModel.transform(test)\n",
    "dtPredictions.select('features','Price','prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have built a model, we need to evaluate how well it performs. In spark.ml there are classification, regression, clustering, and ranking evaluators. Given that this is a regression problem, we will use rootmean- square error (RMSE) and R2 (pronounced “R-squared”) to evaluate our model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSE\n",
    "Evaluator for Regression, which expects input columns prediction, label and an optional weight column. RMSE is a metric that ranges from zero to infinity. The closer it is to zero, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 347450.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "predictionCol=\"prediction\",\n",
    "labelCol=\"Price\",\n",
    "metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(dtPredictions)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R2 (R-Squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.7188016401964463\n"
     ]
    }
   ],
   "source": [
    "# Calculating the R-squared value\n",
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(dtPredictions)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R squared at 0.72 indicates that in our model, approximate 72% of the variability in “Price” can be explained using the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pipeline estimator\n",
    "pipeline = Pipeline(stages = [inputIndexer,encoder,assembler, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "# Specifying the hyperparameters and their respective values using the ParamGridBuilder\n",
    "paramGrid = (ParamGridBuilder()\n",
    ".addGrid(dt.maxDepth, [2, 4, 6])\n",
    ".addGrid(dt.maxBins, [10, 20, 40, 100]) \n",
    "# maxBins determines the number of bins into which your continuous features are discretized, or split\n",
    ".build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining how to evaluate each of the models to determine which one perfroms best\n",
    "# Using RegressionEvaluator and RMSE as the metric\n",
    "evaluator = RegressionEvaluator(labelCol=\"Price\",\n",
    "predictionCol=\"prediction\",\n",
    "metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform the k-fold cross-validation using the CrossValidator, which accepts\n",
    "an estimator, evaluator, and estimatorParamMaps so that it knows which model to\n",
    "use, how to evaluate the model, and which hyperparameters to set for the model. We\n",
    "also set the number of folds we want to split the data into (numFolds=3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=dt,\n",
    "evaluator=evaluator,\n",
    "estimatorParamMaps=paramGrid,\n",
    "numFolds=3,\n",
    "parallelism = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidatorModel_f343a9de7b27\n"
     ]
    }
   ],
   "source": [
    "# Fitting the cvModel to train data set\n",
    "cvModel = cv.fit(train)\n",
    "print(cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------------+\n",
      "|            features|    Price|        prediction|\n",
      "+--------------------+---------+------------------+\n",
      "|(770,[0,40,327,33...|1315000.0| 838275.8362731795|\n",
      "|(770,[0,40,327,33...| 711000.0| 838275.8362731795|\n",
      "|(770,[0,40,327,33...|1051000.0|1289384.8909001958|\n",
      "|(770,[0,40,327,33...| 590069.0|1289384.8909001958|\n",
      "|(770,[0,40,327,33...| 573000.0| 893368.1394891945|\n",
      "|(770,[0,40,330,33...| 545000.0| 893368.1394891945|\n",
      "|(770,[0,40,328,39...|1225000.0| 893368.1394891945|\n",
      "|(770,[0,40,327,34...| 887500.0| 893368.1394891945|\n",
      "|(770,[0,3,327,341...| 707500.0| 838275.8362731795|\n",
      "|(770,[0,3,328,363...| 801000.0| 838275.8362731795|\n",
      "+--------------------+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on test data set\n",
    "dtPredictions = bestModel.transform(test)\n",
    "dtPredictions.select('features','Price','prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.741979570250014\n"
     ]
    }
   ],
   "source": [
    "# Calculating the R-squared value\n",
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(dtPredictions)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are ensembles of decision trees. Random forests combine many decision trees in order to reduce the risk of overfitting. The spark.ml implementation supports random forests for binary and multiclass classification and for regression, using both continuous and categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests truly demonstrate the power of distributed machine learning with\n",
    "Spark, as each tree can be built independently of the other trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"Price\", featuresCol=\"features\")\n",
    "rfModel = rf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------------+\n",
      "|            features|    Price|       prediction|\n",
      "+--------------------+---------+-----------------+\n",
      "|(770,[0,40,327,33...|1315000.0|945188.6778585704|\n",
      "|(770,[0,40,327,33...| 711000.0|921398.0813243687|\n",
      "+--------------------+---------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfPredictions = rfModel.transform(test)\n",
    "rfPredictions.select('features','Price','prediction').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 341010.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "predictionCol=\"prediction\",\n",
    "labelCol=\"Price\",\n",
    "metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(rfPredictions)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.7291293179977745\n"
     ]
    }
   ],
   "source": [
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(rfPredictions)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R squared at 0.73 indicates that in our model, approximate 73% of the variability in “Price” can be explained using the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stages = [inputIndexer,encoder,assembler, rf]\n",
    "pipeline = Pipeline(stages = [inputIndexer,encoder,assembler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = (ParamGridBuilder()\n",
    ".addGrid(rf.maxDepth, [2, 4, 6])\n",
    ".addGrid(rf.numTrees, [10, 100])\n",
    ".build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"Price\",\n",
    "predictionCol=\"prediction\",\n",
    "metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=rf,\n",
    "evaluator=evaluator,\n",
    "estimatorParamMaps=paramGrid,\n",
    "numFolds=3,\n",
    "parallelism = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidatorModel_b362523087b7\n"
     ]
    }
   ],
   "source": [
    "cvModel = cv.fit(train)\n",
    "print(cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+------------------+\n",
      "|            features|    Price|        prediction|\n",
      "+--------------------+---------+------------------+\n",
      "|(770,[0,40,327,33...|1315000.0| 960263.5144604783|\n",
      "|(770,[0,40,327,33...| 711000.0| 958035.6614290236|\n",
      "|(770,[0,40,327,33...|1051000.0|1192403.0944232033|\n",
      "|(770,[0,40,327,33...| 590069.0|1192403.0944232033|\n",
      "|(770,[0,40,327,33...| 573000.0|1103173.6755854357|\n",
      "|(770,[0,40,330,33...| 545000.0|1103173.6755854357|\n",
      "|(770,[0,40,328,39...|1225000.0|1131312.6823927257|\n",
      "|(770,[0,40,327,34...| 887500.0|1131312.6823927257|\n",
      "|(770,[0,3,327,341...| 707500.0| 796535.8046203809|\n",
      "|(770,[0,3,328,363...| 801000.0| 805351.2148404751|\n",
      "+--------------------+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the model on test data set\n",
    "rfPredictions = bestModel.transform(test)\n",
    "rfPredictions.select('features','Price','prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.7556648858805033\n"
     ]
    }
   ],
   "source": [
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(rfPredictions)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-boosted tree regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient-Boosted Trees (GBTs) are ensembles of decision trees. GBTs iteratively train decision trees in order to minimize a loss function. The spark.ml implementation supports GBTs for binary classification and for regression, using both continuous and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "# Create an initial model using the train set.\n",
    "gbt = GBTRegressor(featuresCol = 'features', labelCol = 'Price', maxIter=10)\n",
    "gbtModel = gbt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----------------+\n",
      "|            features|    Price|       prediction|\n",
      "+--------------------+---------+-----------------+\n",
      "|(770,[0,40,327,33...|1315000.0|818475.2469973344|\n",
      "|(770,[0,40,327,33...| 711000.0|818475.2469973344|\n",
      "+--------------------+---------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gbtPredictions = gbtModel.transform(test)\n",
    "gbtPredictions.select('features','Price','prediction').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 316104.4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(\n",
    "predictionCol=\"prediction\",\n",
    "labelCol=\"Price\",\n",
    "metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(gbtPredictions)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.7672507750229816\n"
     ]
    }
   ],
   "source": [
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(gbtPredictions)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R squared at 0.77 indicates that in our model, approximate 77% of the variability in “Price” can be explained using the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = [inputIndexer,encoder,assembler, gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "paramGrid = (ParamGridBuilder()\n",
    ".addGrid(gbt.maxDepth, [2, 4, 6])\n",
    ".addGrid(gbt.maxBins, [10, 20, 40, 100])\n",
    ".build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"Price\",\n",
    "predictionCol=\"prediction\",\n",
    "metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=gbt,\n",
    "evaluator=evaluator,\n",
    "estimatorParamMaps=paramGrid,\n",
    "numFolds=3,\n",
    "parallelism = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidatorModel_dfbc81c7e48b\n"
     ]
    }
   ],
   "source": [
    "cvModel = cv.fit(train)\n",
    "print(cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model on test data set\n",
    "gbtPredictions = bestModel.transform(test)\n",
    "gbtPredictions.select('features','Price','prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.7719249211850375\n"
     ]
    }
   ],
   "source": [
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(gbtPredictions)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation and hyperparameter tuning does make difference in determining which model performs best. the two models such as decision tree, random forest predictive performance was improved noticeably, where as for the gradient-boosted tree model the result was not changed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
